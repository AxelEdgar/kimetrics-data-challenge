# Data Engineer II Challenge - Retail Analytics Platform (PostgreSQL + AWS RDS)

## "De los Datos Crudos a la Decisi√≥n de Negocio: Una Plataforma Anal√≠tica Optimizada para el Retail"

---

### **Resumen Ejecutivo del Proyecto**

Este repositorio contiene la soluci√≥n completa al desaf√≠o de Data Engineer II. El proyecto consiste en el dise√±o, implementaci√≥n y despliegue de un **modelo de datos anal√≠tico (OLAP)** para el sector retail, utilizando **PostgreSQL** sobre una infraestructura gestionada de **AWS RDS (Free Tier)**.

La soluci√≥n es integral y abarca todo el ciclo de vida de un proyecto de datos:

1. **Generaci√≥n de Datos a Gran Escala:** Creaci√≥n de un dataset que supera ampliamente los requisitos del reto. Se generaron m√°s de **12.9 millones de registros de ventas** para simular patrones de negocio realistas y realizar pruebas de rendimiento significativas.
2. **Modelado de Datos Dimensional:** Dise√±o de un **esquema en estrella optimizado**, el est√°ndar de la industria para consultas anal√≠ticas de alto rendimiento.
3. **Ingenier√≠a de Datos (DDL/DML):** Construcci√≥n de la base de datos con scripts SQL robustos, incluyendo **particionamiento de tablas**, constraints de integridad y procesos de carga masiva de datos (`\copy`).
4. **Optimizaci√≥n Extrema de Rendimiento:** Un an√°lisis profundo de consultas complejas, aplicando t√©cnicas avanzadas como **Vistas Materializadas** e √≠ndices estrat√©gicos para lograr mejoras de rendimiento de hasta un **99.9%**, reduciendo tiempos de ejecuci√≥n de minutos a milisegundos.
5. **Documentaci√≥n Exhaustiva y Profesional:** Creaci√≥n de artefactos clave como un Modelo Entidad-Relaci√≥n (MER), un diccionario de datos detallado y un documento de decisiones de arquitectura para garantizar la mantenibilidad y escalabilidad del sistema.

El proyecto demuestra la capacidad de construir una soluci√≥n de datos de extremo a extremo, aplicando las mejores pr√°cticas en modelado, rendimiento y documentaci√≥n t√©cnica.

---

### **Tabla de Contenidos**

1. [üéØ Visi√≥n y Objetivos del Proyecto](#-visi√≥n-y-objetivos-del-proyecto)
2. [üìä Casos de Negocio Implementados](#-casos-de-negocio-implementados)
3. [üöÄ Gu√≠a R√°pida de Despliegue (Quick Start)](#-gu√≠a-r√°pida-de-despliegue-quick-start)
4. [üìÅ Estructura Detallada del Repositorio](#-estructura-detallada-del-repositorio)
5. [üèóÔ∏è Arquitectura Profunda del Modelo](#Ô∏è-arquitectura-profunda-del-modelo)
6. [‚ö° An√°lisis de Optimizaci√≥n de Performance](#-an√°lisis-de-optimizaci√≥n-de-performance)
7. [üìà L√≥gica de Generaci√≥n de Datos](#-l√≥gica-de-generaci√≥n-de-datos)
8. [üîß Despliegue en AWS RDS](#-despliegue-en-aws-rds)
9. [üîç Protocolos de Validaci√≥n y Calidad de Datos](#-protocolos-de-validaci√≥n-y-calidad-de-datos)
10. [üìö Documentaci√≥n del Proyecto](#-documentaci√≥n-del-proyecto)
11. [üîÆ Trabajo Futuro y Escalabilidad](#-trabajo-futuro-y-escalabilidad)
12. [üìß Contacto](#-contacto)

---

## üéØ Visi√≥n y Objetivos del Proyecto

El objetivo principal es **dise√±ar e implementar un modelo anal√≠tico de datos para retail que sea escalable, performante y mantenible**, utilizando PostgreSQL y desplegado en un entorno de nube realista como AWS RDS.

### Objetivos Secundarios

- **Demostrar maestr√≠a en modelado dimensional**, aplicando el concepto de esquema en estrella para optimizar consultas OLAP.
- **Probar la capacidad de generar datos sint√©ticos de alta calidad** que imiten la complejidad del mundo real, sirviendo como una base s√≥lida para pruebas de rendimiento.
- **Aplicar y cuantificar t√©cnicas avanzadas de optimizaci√≥n de bases de datos** en PostgreSQL, demostrando c√≥mo reducir dr√°sticamente la latencia de las consultas.
- **Producir documentaci√≥n t√©cnica de nivel profesional** que permita a otros ingenieros entender, utilizar y extender el sistema f√°cilmente.
- **Validar la soluci√≥n en un entorno cloud (AWS RDS)**, considerando aspectos pr√°cticos como la configuraci√≥n, la seguridad y el monitoreo.

---

## üìä Casos de Negocio Implementados

El modelo de datos fue construido para responder a preguntas de negocio cr√≠ticas. A continuaci√≥n se detallan los casos de uso principales y c√≥mo el modelo los habilita.

### 1. Dashboard Ejecutivo: Ventas Diarias

- **Valor de Negocio:** Proporciona a la alta direcci√≥n una vista en tiempo real del rendimiento de ventas por cadena y tienda, permitiendo una toma de decisiones √°gil.
- **Implementaci√≥n:** Se utiliza una vista materializada (`mv_daily_sales_summary`) que pre-calcula las ventas diarias, los tickets promedio y las comparativas con el d√≠a anterior.
- **Consulta de Ejemplo:**

  ```sql
  -- Obtener KPIs de ventas para la √∫ltima semana para la cadena 'HyperMarket'
  SELECT
      fecha,
      cadena,
      total_ventas,
      numero_tickets,
      ticket_promedio
  FROM retail.mv_daily_sales_summary
  WHERE
      fecha >= CURRENT_DATE - INTERVAL '7 days' AND
      cadena = 'HyperMarket'
  ORDER BY fecha DESC;
  ```

### 2. Merchandising: Top Productos por Regi√≥n

- **Valor de Negocio:** Permite a los equipos de merchandising y marketing identificar qu√© productos son m√°s populares en cada regi√≥n, optimizando as√≠ el surtido de inventario y las campa√±as publicitarias localizadas.
- **Implementaci√≥n:** Una vista materializada (`mv_product_performance`) calcula el ranking de ventas de cada producto dentro de su categor√≠a y regi√≥n.
- **Consulta de Ejemplo:**

  ```sql
  -- Encontrar los 5 productos de 'Electr√≥nica' m√°s vendidos en la regi√≥n 'Norte' durante el √∫ltimo mes
  SELECT
      p.nombre_producto,
      p.marca,
      perf.total_unidades_vendidas,
      perf.total_ingresos
  FROM retail.mv_product_performance perf
  JOIN retail.productos p ON perf.producto_id = p.producto_id
  WHERE
      perf.region = 'Norte' AND
      p.categoria = 'Electr√≥nica' AND
      perf.ranking_categoria_region <= 5
  ORDER BY perf.total_ingresos DESC;
  ```

### 3. Pricing: Ticket Promedio por Formato

- **Valor de Negocio:** Ayuda a los analistas de precios a entender el comportamiento de compra en diferentes formatos de tienda (ej. Hipermercado vs. Tienda de Conveniencia), fundamental para ajustar estrategias de precios y promociones.
- **Implementaci√≥n:** La consulta agrega datos de la tabla de ventas y la dimensi√≥n de tiendas.
- **Consulta de Ejemplo:**

  ```sql
  -- Calcular el ticket promedio y el n√∫mero de art√≠culos por ticket para cada formato de tienda
  SELECT
      t.formato,
      AVG(v.total_linea) AS ingreso_promedio_por_linea,
      SUM(v.total_linea) / COUNT(DISTINCT v.ticket_id) AS ticket_promedio,
      SUM(v.unidades) / COUNT(DISTINCT v.ticket_id) AS articulos_por_ticket
  FROM retail.ventas v
  JOIN retail.tiendas t ON v.tienda_id = t.tienda_id
  WHERE v.fecha_venta BETWEEN '2025-01-01' AND '2025-03-31'
  GROUP BY t.formato
  ORDER BY ticket_promedio DESC;
  ```

### 4. Operaciones: Alertas de Inventario

- **Valor de Negocio:** Permite una gesti√≥n proactiva del inventario, identificando productos con riesgo de quiebre de stock (`stockout`) o con exceso de inventario (`overstock`), optimizando el capital de trabajo.
- **Implementaci√≥n:** Una vista materializada (`mv_inventory_alerts`) compara el stock actual con umbrales predefinidos (ej. stock de seguridad).
- **Consulta de Ejemplo:**

  ```sql
  -- Identificar todos los productos en la cadena 'Supermercado' con alerta de 'STOCK_BAJO' o 'SIN_STOCK'
  SELECT
      t.nombre_tienda,
      p.nombre_producto,
      inv.stock_actual,
      inv.alerta_inventario
  FROM retail.mv_inventory_alerts inv
  JOIN retail.tiendas t ON inv.tienda_id = t.tienda_id
  JOIN retail.productos p ON inv.producto_id = p.producto_id
  WHERE
      t.cadena = 'Supermercado' AND
      inv.alerta_inventario IN ('SIN_STOCK', 'STOCK_BAJO');
  ```

---

## üöÄ Gu√≠a R√°pida de Despliegue (Quick Start)

Esta secci√≥n proporciona los pasos necesarios para clonar, configurar y ejecutar el proyecto completo.

### Prerrequisitos

- **Git:** Para clonar el repositorio.
- **Python 3.9+ y pip:** Para ejecutar el script de generaci√≥n de datos.
- **PostgreSQL Client (`psql`):** Para interactuar con la base de datos desde la l√≠nea de comandos.
- **Acceso a una instancia de PostgreSQL:** Puede ser una instalaci√≥n local, una instancia en Docker o una base de datos en la nube como AWS RDS.

### Pasos de Ejecuci√≥n

```bash
# PASO 1: Clonar el repositorio y navegar a la carpeta del proyecto.
# ----------------------------------------------------------------
git clone https://github.com/tu-usuario/data-engineer-challenge.git
cd data-engineer-challenge

# PASO 2: Crear un entorno virtual de Python e instalar las dependencias.
# Es una buena pr√°ctica aislar las dependencias del proyecto.
# ----------------------------------------------------------------
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
pip install -r requirements.txt

# PASO 3: Generar los datos dummy.
# Este script crear√° varios archivos .csv en la carpeta /data.
# Puede tardar varios minutos debido al gran volumen de datos (+12.9M de filas).
# ----------------------------------------------------------------
echo "Iniciando la generaci√≥n de datos... Esto puede tardar."
python 00_generate_data.py
echo "Datos generados exitosamente en la carpeta /data."

# PASO 4: Configurar las variables de conexi√≥n a la base de datos.
# Se recomienda usar variables de entorno para no exponer credenciales.
# ----------------------------------------------------------------
export HOST="your-rds-endpoint"
export USER="your_user"
export DBNAME="retail"
# Se te pedir√° la contrase√±a de forma interactiva al ejecutar psql.

# PASO 5: Ejecutar los scripts SQL en el orden secuencial correcto.
# El orden es cr√≠tico para asegurar que las dependencias (tablas, datos) se cumplan.
# ----------------------------------------------------------------

# 5.a: Crear el esquema, las tablas, las particiones y las secuencias.
echo "Ejecutando DDL: 01_create_schema.sql"
psql -h $HOST -U $USER -d $DBNAME -f sql/01_create_schema.sql

# 5.b: Poblar la dimensi√≥n de calendario, que es independiente de los datos generados.
echo "Poblando Dimensi√≥n Calendario: 03_populate_calendar.sql"
psql -h $HOST -U $USER -d $DBNAME -f sql/03_populate_calendar.sql

# 5.c: Cargar los datos masivamente usando el comando \copy de psql. Es altamente eficiente.
echo "Cargando Datos Masivos: 04_load_data.sql"
psql -h $HOST -U $USER -d $DBNAME -f sql/04_load_data.sql

# 5.d: Crear los √≠ndices despu√©s de la carga de datos para acelerar el proceso de ingesta.
echo "Creando √çndices Optimizados: 02_create_indexes.sql"
psql -h $HOST -U $USER -d $DBNAME -f sql/02_create_indexes.sql

# 5.e: Crear las vistas materializadas para pre-agregar datos y acelerar los dashboards.
echo "Creando Vistas Materializadas: materialized_views.sql"
psql -h $HOST -U $USER -d $DBNAME -f explain/materialized_views.sql

echo "¬°Despliegue completado exitosamente!"
```

---

## ‚ö° An√°lisis de Optimizaci√≥n de Performance

Se logr√≥ una mejora dr√°stica en el rendimiento de consultas anal√≠ticas complejas mediante la implementaci√≥n de **Vistas Materializadas**, **Particionamiento de Tablas** e **√çndices Estrat√©gicos**.

### Metodolog√≠a de Benchmarking

- **Entorno:** Instancia `db.t3.micro` de AWS RDS con PostgreSQL 15.
- **Datos:** Tabla `fact_ventas` con +12.9 millones de registros.
- **Proceso:** Se ejecut√≥ cada consulta 5 veces usando `EXPLAIN ANALYZE` antes y despu√©s de las optimizaciones. Se reporta el tiempo de ejecuci√≥n promedio.

### Resultados Cuantitativos

| Consulta Anal√≠tica        | Tiempo Antes (ms) | Tiempo Despu√©s (ms) | Mejora (%)  | √ìrdenes de Magnitud de Mejora |
| ------------------------- | ----------------- | ------------------- | ----------- | ----------------------------- |
| Top Productos por Regi√≥n  | 5,415.03          | 4.70                | **99.9%**   | ~1150x m√°s r√°pida             |
| An√°lisis de Inventario    | 14,582.68         | 22.65               | **99.8%**   | ~645x m√°s r√°pida              |
| Ventas Diarias por Cadena | 11,195.37         | 4,496.20            | **59.8%**   | ~2.5x m√°s r√°pida              |

*Los an√°lisis detallados, planes de ejecuci√≥n (`EXPLAIN ANALYZE`) y las consultas optimizadas se encuentran en la carpeta `/explain`.*

### T√©cnicas Aplicadas en Detalle

- **Vistas Materializadas:** En lugar de calcular agregaciones complejas (SUM, COUNT, RANK) sobre millones de filas en cada ejecuci√≥n, estas vistas pre-calculan y almacenan f√≠sicamente los resultados. Consultar la vista es casi instant√°neo. Es la t√©cnica de mayor impacto para dashboards y reportes recurrentes.
- **Particionamiento (Partition Pruning):** Al filtrar por fecha, el motor de PostgreSQL ni siquiera lee las particiones de a√±os que no est√°n en el rango del filtro. Esto reduce dr√°sticamente la cantidad de datos que deben ser procesados en disco (I/O).
- **√çndices Covering y Compuestos:** Se crearon √≠ndices multicolumna (`Composite Indexes`) en las llaves for√°neas y columnas de filtro m√°s comunes (`tienda_id`, `producto_id`, `fecha_venta`). Esto permite al planificador de consultas encontrar los datos de manera extremadamente r√°pida sin tener que escanear la tabla completa (`Full Table Scan`).

---

## üìà L√≥gica de Generaci√≥n de Datos

Para asegurar que las pruebas de rendimiento fueran v√°lidas y representativas, se puso especial √©nfasis en la generaci√≥n de un dataset realista y a gran escala.

### Volumen y Dimensiones

Mientras que el desaf√≠o especificaba un m√≠nimo de 1,000 registros por a√±o, esta soluci√≥n va significativamente m√°s all√° para simular un escenario de producci√≥n. El script `00_generate_data.py` genera:

- **Tabla de Ventas:** **~12.9 millones** de transacciones distribuidas a lo largo de 5 a√±os.
- **Dimensiones:**
  - 125 productos √∫nicos en 8 categor√≠as.
  - 325 tiendas en 4 cadenas, 6 regiones y 4 formatos distintos.
  - 5 a√±os completos de datos en la dimensi√≥n de calendario.

### Simulaci√≥n de Patrones de Negocio

Los datos no son puramente aleatorios. Se introdujeron patrones para imitar el comportamiento real del consumidor:

- **Estacionalidad:** Un claro pico de ventas en Diciembre y ca√≠das en Enero/Febrero.
- **Patrones Semanales:** Mayor volumen de transacciones los viernes y s√°bados.
- **Rendimiento por Formato:** Los hipermercados tienen tickets promedio m√°s altos que las tiendas de conveniencia.
- **Distribuci√≥n de Canales:** 85% de las ventas en tienda f√≠sica, 12% online y 3% a trav√©s de la app m√≥vil.

---

## üìÅ Estructura Detallada del Repositorio

```text
data-engineer-challenge/
‚îÇ
‚îú‚îÄ‚îÄ 00_generate_data.py             # Script principal en Python para generar todos los archivos CSV.
‚îú‚îÄ‚îÄ requirements.txt                # Lista de librer√≠as Python necesarias (Pandas, Faker).
‚îú‚îÄ‚îÄ .gitignore                      # Ignora archivos que no deben ser versionados (ej. /data, /venv).
‚îÇ
‚îú‚îÄ‚îÄ sql/                            # Directorio con todos los scripts de lenguaje de definici√≥n de datos (DDL).
‚îÇ   ‚îú‚îÄ‚îÄ 01_create_schema.sql        # CREA el esquema, las tablas principales, las particiones y las secuencias. Define PKs y FKs.
‚îÇ   ‚îú‚îÄ‚îÄ 02_create_indexes.sql       # CREA los √≠ndices optimizados (B-Tree, Composite) en las tablas de hechos y dimensiones.
‚îÇ   ‚îú‚îÄ‚îÄ 03_populate_calendar.sql    # INSERTA datos en la tabla `dim_calendario` para un rango de 5 a√±os.
‚îÇ   ‚îî‚îÄ‚îÄ 04_load_data.sql            # UTILIZA el comando `\copy` para cargar eficientemente los datos desde los CSV a las tablas.
‚îÇ
‚îú‚îÄ‚îÄ explain/                        # Foco en el an√°lisis y la optimizaci√≥n del rendimiento de consultas.
‚îÇ   ‚îú‚îÄ‚îÄ ... (Archivos Antes/Despu√©s con EXPLAIN ANALYZE)
‚îÇ   ‚îú‚îÄ‚îÄ materialized_views.sql      # DDL para crear todas las vistas materializadas usadas en la optimizaci√≥n.
‚îÇ   ‚îî‚îÄ‚îÄ performance_results.md      # Documento que presenta los resultados y cuantifica las mejoras.
‚îÇ
‚îú‚îÄ‚îÄ docs/                           # Toda la documentaci√≥n conceptual y de dise√±o del proyecto.
‚îÇ   ‚îú‚îÄ‚îÄ MER.png                     # Diagrama visual del Modelo Entidad-Relaci√≥n (Esquema en Estrella).
‚îÇ   ‚îú‚îÄ‚îÄ diccionario_datos.md        # Descripci√≥n detallada de cada tabla, columna, tipo de dato y su prop√≥sito.
‚îÇ   ‚îú‚îÄ‚îÄ architecture_decisions.md   # Justificaciones t√©cnicas de las decisiones clave de dise√±o.
‚îÇ
‚îî‚îÄ‚îÄ data/                           # (Generado localmente, no versionado) Almacena los archivos CSV de datos dummy.
```

---

## üèóÔ∏è Arquitectura Profunda del Modelo

### Filosof√≠a del Dise√±o: El Esquema en Estrella

- **¬øPor qu√© un esquema en estrella?**
  - **Simplicidad:** El modelo es f√°cil de entender para los analistas de negocio. Las consultas son m√°s intuitivas.
  - **Rendimiento:** Las consultas de agregaci√≥n son inherentemente m√°s r√°pidas.
  - **Optimizaci√≥n para Lectura:** Este modelo est√° optimizado para la lectura y agregaci√≥n masiva de datos.

### Pila Tecnol√≥gica y Justificaci√≥n

- **Base de Datos: PostgreSQL 15+**
  - **¬øPor qu√©?** Es una base de datos de c√≥digo abierto, robusta, madura y con un soporte excelente para funcionalidades anal√≠ticas avanzadas.
- **Generaci√≥n de Datos: Python + Pandas + Faker**
  - **¬øPor qu√©?** La combinaci√≥n l√≠der en la industria para la manipulaci√≥n de datos y la generaci√≥n de datos sint√©ticos realistas.
- **Despliegue: AWS RDS Free Tier**
  - **¬øPor qu√©?** Permite validar la soluci√≥n en un entorno de nube real sin incurrir en costos, demostrando la viabilidad del despliegue en producci√≥n.

---

## üîß Despliegue en AWS RDS

### Configuraci√≥n de la Instancia

```yaml
Instance Class: db.t3.micro (Parte del AWS Free Tier)
Storage: 20 GB General Purpose SSD (gp2)
DB Engine: PostgreSQL 15.x
Multi-AZ Deployment: No (Para mantenerse dentro del Free Tier)
Public Access: Yes (Habilitado temporalmente para validaci√≥n del revisor)
```

### Mejores Pr√°cticas de Seguridad

- **Usuario con Privilegios M√≠nimos:** Se cre√≥ un usuario `kimetrics_reviewer` con permisos de solo `SELECT` en el esquema `retail`.
- **Security Group Restringido:** El grupo de seguridad de la instancia RDS deber√≠a estar configurado para permitir el tr√°fico entrante en el puerto 5432 solo desde la IP del revisor.
- **Gesti√≥n de Credenciales:** Las credenciales no est√°n hardcodeadas en los scripts; se utilizan variables de entorno.
- **Post-Validaci√≥n:** El acceso p√∫blico a la instancia debe ser deshabilitado una vez que la revisi√≥n haya concluido.

---

## üîç Protocolos de Validaci√≥n y Calidad de Datos

- **Integridad Referencial:** Se utilizan `FOREIGN KEY` constraints para asegurar que cada venta o registro de inventario se relacione con un producto y una tienda existentes.
- **Unicidad de Claves:** `PRIMARY KEY` en todas las tablas y constraints `UNIQUE` en c√≥digos de negocio (SKU, c√≥digo de tienda) para prevenir duplicados.
- **Validaci√≥n de Rangos:** Constraints `CHECK` aseguran que los valores num√©ricos como precios y cantidades sean siempre positivos.
- **Calidad de la Carga:** El proceso de carga masiva asegura que el 100% de los campos obligatorios (`NOT NULL`) est√©n poblados.

---

## üìö Documentaci√≥n del Proyecto

Para una comprensi√≥n m√°s profunda del proyecto, consulta los siguientes documentos en la carpeta `/docs`:

- **[Modelo Entidad-Relaci√≥n (MER)](docs/MER.png)**: Diagrama visual de la arquitectura del modelo de datos.
- **[Diccionario de Datos](docs/diccionario_datos.md)**: Descripci√≥n detallada de cada tabla, columna, tipo de dato y su prop√≥sito de negocio.
- **[Decisiones de Arquitectura](docs/architecture_decisions.md)**: Justificaci√≥n t√©cnica de las elecciones de dise√±o.

---

## üîÆ Trabajo Futuro y Escalabilidad

Aunque esta soluci√≥n es robusta, aqu√≠ hay algunas v√≠as para su evoluci√≥n:

- **Orquestaci√≥n de ETL/ELT:** Integrar una herramienta como **Apache Airflow** o **dbt (Data Build Tool)** para automatizar, programar y monitorear la ejecuci√≥n de los scripts de carga y la actualizaci√≥n de las vistas materializadas.
- **Escalado a un Data Warehouse Cloud-Nativo:** Si el volumen de datos creciera a Terabytes, el siguiente paso l√≥gico ser√≠a migrar el modelo a una plataforma como **Amazon Redshift**, **Google BigQuery** o **Snowflake**, que est√°n dise√±adas para el procesamiento masivo en paralelo.
- **Infraestructura como C√≥digo (IaC):** Utilizar **Terraform** para definir y provisionar la instancia de RDS y la configuraci√≥n de seguridad, permitiendo despliegues consistentes y reproducibles.
- **CI/CD para la Base de Datos:** Implementar un pipeline de CI/CD (ej. con GitHub Actions) que pruebe y despliegue autom√°ticamente los cambios en los scripts SQL a diferentes entornos (desarrollo, producci√≥n).

---

## üìß Contacto

**Desarrollado para:** Kimetrics Data Engineer II Challenge
**Repositorio:** https://github.com/AxelEdgar/kimetrics-data-challenge.git

**Documentaci√≥n Adicional:** Ver la carpeta `/docs` para detalles t√©cnicos completos.
